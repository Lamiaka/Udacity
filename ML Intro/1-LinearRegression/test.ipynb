import pandas as pd
import numpy as np
import time
import itertools
import os

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression, Perceptron
from sklearn.svm import LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, mean_squared_error, \
r2_score, f1_score, recall_score, roc_curve, auc, confusion_matrix
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import GridSearchCV

import matplotlib.pyplot as plt
from sklearn.tree import export_graphviz

DEFINE THE FUNCTIONS

def test_classifiers(alg_list, X_train, y_train, X_test, y_test):
    classifier_name_list = []
    # initialize an empty data frame to store the performance of multiple algorithms
    classifier_stats = pd.DataFrame()
    # for each of the classifiers in alg list
    for (clf, i) in zip(alg, range(len(alg))):
        classifier_name = str(clf).split('(')[0]
        try:
            if clf.class_weight == 'balanced':
                classifier_name = classifier_name + ' balanced'
        except: pass
        print '\n\n'
        print 'Training', classifier_name, '...'
        # start measuring time
        start = time.time()
        # train the classifier
        clf.fit(X_train[model_columns], y_train)
        # stop measuring time
        end = time.time()
        # calculate how much time is needed to train the classifier
        t = round((end - start),2)
        # make a prediction on the test dataset
        y_pred = clf.predict(X_test[model_columns])
        # calculate the metrics - here weighted f1 score and recall for the class 1
        score = round(f1_score(y_test,y_pred, average='weighted'),2)
        recall = round(recall_score(y_test,y_pred),2)
        # print the time needed to train the classifier
        print 'Time needed for', classifier_name ,'is:', t, 's'
        # print some learning metrics
        print 'Classification report:'
        # calculate precision, recall and F1 score
        print classification_report(y_test,y_pred,target_names = ['non-default','default'])
        try:
            prediction_indication = clf.predict_proba(X_test[model_columns])[:,1]
        except:
            prediction_indication = clf.decision_function(X_test[model_columns])
        fpr, tpr, thr = roc_curve(y_test, prediction_indication)
        roc_auc = round(auc(fpr, tpr),2)
        # append the classifier list with a name of the classifier
        classifier_name_list.append(classifier_name)
        # append the performance data frame with the name of the classifier
        # the performance score and the time needed to train it
        classifier_stats.set_value(i, 'name', classifier_name)
        classifier_stats.set_value(i, 'weighted f1-score', score)
        classifier_stats.set_value(i, 'class 1 recall', recall)
        classifier_stats.set_value(i, 'AUC_score', roc_auc)
        classifier_stats.set_value(i, 'time', t)
        classifier_stats.set_value(i, 'param', clf)

        plt.figure(1, figsize = (20,10))
        plt.plot([0, 1], [0, 1], 'k--')
        plt.plot(fpr, tpr, label=classifier_name)

    plt.xlabel('False positive rate', size=18)
    plt.xticks(size=16)
    plt.ylabel('True positive rate', size=18)
    plt.yticks(size=16)
    plt.title('ROC curve', size = 24)
    plt.legend(loc='best', labels = classifier_name_list)
    plt.show()

    return classifier_stats

    
    
def metrics_for_train_and_test(classifier, X_train, y_train, X_test, y_test, model_columns):
    # calculate the y_pred and Y_pred proba for the training and test set
    y_pred_train = classifier.predict(X_train[model_columns])
    y_pred = classifier.predict(X_test[model_columns])

    
    # calculate accuracy, AUC and plot the classification report
    print 'Metrics for the training set:'
    print 'Accuracy:', round(accuracy_score(y_train, y_pred_train),2)
    try:
        prediction_indication_train = classifier.predict_proba(X_train[model_columns])[:,1]
    except: 
        prediction_indication_train = classifier.decision_function(X_train[model_columns])
    print 'AUC:', round(roc_auc_score(y_train, prediction_indication_train),2)
    print classification_report(y_train, y_pred_train)

    
    print '\n\nMetrics for the test set:'
    print 'Accuracy:', round(accuracy_score(y_test, y_pred),2)
    try:
        prediction_indication_test = classifier.predict_proba(X_test[model_columns])[:,1]
    except:
        prediction_indication_test = classifier.decision_function(X_test[model_columns])
    print 'AUC:', round(roc_auc_score(y_test, prediction_indication_test),2)
    print classification_report(y_test, y_pred)
    
    return y_pred
    
    
def plot_feature_importance(classifier, model_columns):
    importances = classifier.feature_importances_
    indices = np.argsort(importances)[::-1]
    feature_names_sorted = [model_columns [i] for i in indices]

    plt.figure()
    plt.figure(figsize = (20,10))
    plt.title("Feature importances", size = 24)
    plt.bar(range(10), importances[indices][0:10], color = 'b', align = 'center')
    plt.xticks(range(10), feature_names_sorted[0:10], rotation = 90, size=18)
    plt.yticks(size=18)
    plt.xlim([-1, 11])
    plt.show()
    
def plot_confusion_matrix(y_test, y_pred, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.PuRd):
    
    cm = confusion_matrix(y_test, y_pred)

    plt.figure()
    plt.figure(figsize = (10,5))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title, size=20)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label', size=18)
    plt.xlabel('Predicted label', size=18)
    plt.show()
    
    
def estimate_profit_and_cost(X_test, y_test, y_pred, FP_cost):

    # group the predicted buyers based on their buyer_segment
    buyer_segment_pred = X_test[((y_test == 1) & (y_pred == 1))].groupby('buyer_segment').count()['action_ID']
    ### print buyer_segment_pred
    ### print '\n'
    ### print purchased_smth_grouped
    # create a dataframe with calculated earlier purchase estimates per buyer group
    purchase_estimates = pd.DataFrame(dict(number_of_buyers=buyer_segment_pred, \
                                           average_purchase_amount=purchased_smth_grouped)).reset_index()
    ### print '\n'
    ### print purchase_estimates
    # multiply the number of (correctly) classified cases per segment per average amount spent per group
    purchase_estimates['estimated_profit'] = purchase_estimates['average_purchase_amount']*purchase_estimates['number_of_buyers']
    ### print '\n'
    ### print purchase_estimates
    ### print '\n'
    print 'Number of correctly classified potential buyers:', buyer_segment_pred.sum()
    print 'Total estimated profit is:', purchase_estimates['estimated_profit'].sum()
    ### print '\n'
    # multiply the number of False Positives
    no_purchase_mislabelled = X_test[((y_test == 0) & (y_pred == 1))].shape[0]
    print 'Number of incorrectly classified potential buyers:', no_purchase_mislabelled
    # multiply the number of sellers incorrectly classified as potential buyers per cost of sending them an email
    print 'Total estimated cost due to False Positives is:', no_purchase_mislabelled*FP_cost


import the dataset

    X_train = pd.read_csv('output/X_train.csv', delimiter = ';', index_col=0)
X_test = pd.read_csv('output/X_test.csv', delimiter = ';', index_col=0)

y_train = X_train['purchase']
y_test = X_test['purchase']



4) CLASSIFY
4.1) PREPARE THE FEATURE LIST FOR THE CLASSIFIER
REMOVE THE CATEGORICAL COLUMNS (9), RESPONSE (4), "ACTION_ID" AND "SELLER" COLUMNS

model_columns = X_train.columns.tolist()
print 'Number of columns before the removal:', len(model_columns)

columns_to_be_removed = ['action_ID', 'seller', 'buyer_segment', 'buyer_segment_num', \
'full_category', 'full_category_num', 'category', 'category_num', 'parent_category',\
'parent_category_num', 'full_category_stemmed','after_7d_value','after_7d_purchases', \
'avg_purchase','purchase']

for column in columns_to_be_removed:
    model_columns.remove(column)
print 'Number of columns after the removal:', len(model_columns)



4.2) CALCULATE THE ESTIMATED PROFIT FOR 1% OF THE (CORRECTLY DETECTED) BUYERS
CHECK HOW MUCH PEOPLE SPENT ON AVERAGE IF THEY BUY SOMETHING AFTER THE AUCTION IS TERMINATED

df = pd.concat([X_train, X_test], axis=0)

average_purchase = round(purchased_smth['after_7d_value'].mean(),2)
print 'On average, people spent:', average_purchase


purchased_smth_grouped = purchased_smth.groupby('buyer_segment').mean()['after_7d_value'].round(2)
purchased_smth_grouped

CALCULATE THE ESTIMATED PROFITÂ¶

print 'The estimated profit when detecting 1% of potential buyers is:', round(X_test[y_test == 1].shape[0] * average_purchase / 100)


4.3) TEST DIFFERENT CLASSIFIERS
CLASSIFIERS TO TEST
In [10]:
alg = [SGDClassifier(), SGDClassifier(class_weight = 'balanced'),\
       Perceptron(), Perceptron(class_weight = 'balanced'),\
       LogisticRegression(), LogisticRegression(class_weight = 'balanced'),\
       RandomForestClassifier(), RandomForestClassifier(class_weight = 'balanced'),\
       LinearSVC(), LinearSVC(class_weight='balanced')]#),\
       #GradientBoostingClassifier()]

classifier_stats = test_classifiers(alg, X_train, y_train, X_test, y_test)


Training SGDClassifier ...
Time needed for SGDClassifier is: 3.59 s
Classification report:
             precision    recall  f1-score   support

non-default       0.94      1.00      0.97     65064
    default       0.78      0.13      0.23      4509

avg / total       0.93      0.94      0.92     69573




Training SGDClassifier balanced ...
Time needed for SGDClassifier balanced is: 3.09 s
Classification report:
             precision    recall  f1-score   support

non-default       0.98      0.89      0.93     65064
    default       0.30      0.69      0.42      4509

avg / total       0.93      0.87      0.90     69573




Training Perceptron ...
Time needed for Perceptron is: 2.89 s
Classification report:
             precision    recall  f1-score   support

non-default       0.96      0.96      0.96     65064
    default       0.41      0.41      0.41      4509

avg / total       0.92      0.92      0.92     69573




Training Perceptron balanced ...
Time needed for Perceptron balanced is: 3.51 s
Classification report:
             precision    recall  f1-score   support

non-default       0.98      0.66      0.79     65064
    default       0.14      0.79      0.24      4509

avg / total       0.92      0.67      0.76     69573




Training LogisticRegression ...
Time needed for LogisticRegression is: 12.8 s
Classification report:
             precision    recall  f1-score   support

non-default       0.95      0.99      0.97     65064
    default       0.72      0.24      0.36      4509

avg / total       0.93      0.94      0.93     69573




Training LogisticRegression balanced ...
Time needed for LogisticRegression balanced is: 21.33 s
Classification report:
             precision    recall  f1-score   support

non-default       0.98      0.88      0.92     65064
    default       0.29      0.71      0.41      4509

avg / total       0.93      0.87      0.89     69573




Training RandomForestClassifier ...
Time needed for RandomForestClassifier is: 31.44 s
Classification report:
             precision    recall  f1-score   support

non-default       0.96      1.00      0.98     65064
    default       0.86      0.43      0.57      4509

avg / total       0.95      0.96      0.95     69573




Training RandomForestClassifier balanced ...
Time needed for RandomForestClassifier balanced is: 24.28 s
Classification report:
             precision    recall  f1-score   support

non-default       0.96      1.00      0.98     65064
    default       0.86      0.42      0.56      4509

avg / total       0.95      0.96      0.95     69573




Training LinearSVC ...
Time needed for LinearSVC is: 221.82 s
Classification report:
             precision    recall  f1-score   support

non-default       0.95      0.99      0.97     65064
    default       0.73      0.20      0.31      4509

avg / total       0.93      0.94      0.93     69573




Training LinearSVC balanced ...
Time needed for LinearSVC balanced is: 222.93 s
Classification report:
             precision    recall  f1-score   support

non-default       0.98      0.88      0.92     65064
    default       0.28      0.69      0.40      4509

avg / total       0.93      0.86      0.89     69573


SORT CLASSIFIERS BASED ON THE RECALL FOR CLASS 1



e','class 1 recall','AUC_score','time']]
Out[11]:
name  weighted f1-score class 1 recall  AUC_score time
3 Perceptron balanced 0.76  0.79  0.81  3.51
5 LogisticRegression balanced 0.89  0.71  0.87  21.33
1 SGDClassifier balanced  0.90  0.69  0.87  3.09
9 LinearSVC balanced  0.89  0.69  0.87  222.93
6 RandomForestClassifier  0.95  0.43  0.88  31.44
4.4) PICK THE BEST CLASSIFIER AND EVALUATE ITS PERFORMANCE
In [12]:
classifier = Perceptron(class_weight = 'balanced')
time_start = time.time()
classifier.fit(X_train[model_columns], y_train)
time_stop = time.time()
t = round((time_stop - time_start)/60,2)
print 'Time needed to train the classifier (min):', t
Time needed to train the classifier (min): 0.05
CALCULATE METRICS FOR THE TRAINING AND THE TEST SET
In [13]:
y_pred = metrics_for_train_and_test(classifier, X_train, y_train, X_test, y_test, model_columns)
Metrics for the training set:
Accuracy: 0.67
AUC: 0.8
             precision    recall  f1-score   support

          0       0.98      0.66      0.79    585174
          1       0.14      0.79      0.24     40983

avg / total       0.92      0.67      0.75    626157



Metrics for the test set:
Accuracy: 0.67
AUC: 0.81
             precision    recall  f1-score   support

          0       0.98      0.66      0.79     65064
          1       0.14      0.79      0.24      4509

avg / total       0.92      0.67      0.76     69573

In [14]:
plot_confusion_matrix(y_test, y_pred, classes=['no purchase', 'purchase'],
                      title='Confusion matrix')
<matplotlib.figure.Figure at 0x10e4ab2d0>

CHECK THE BUSINESS POTENTIAL OF THIS CLASSIFIER
In [15]:
estimate_profit_and_cost(X_test, y_test, y_pred, FP_cost=0.1)
Number of correctly classified potential buyers: 3560
Total estimated profit is: 510148.33
Number of incorrectly classified potential buyers: 21815
Total estimated cost due to False Positives is: 2181.5
PLOT FEATURE IMPORTANCE
In [16]:
#plot_feature_importance(classifier, model_columns)
4.5) CHECK IF UNDERSAMPLING WILL HELP
UNDERSAMPLE THE DOMINANT CLASS (0)
In [17]:
purchase_proportion = round(1.0*y_train[y_train == 1].shape[0]/y_train[y_train == 0].shape[0],2)

X_train_0 = X_train[X_train.purchase == 0]
y_train_0 = y_train[X_train.purchase == 0]
X_train_1 = X_train[X_train.purchase == 1]
y_train_1 = y_train[X_train.purchase == 1]

X_train_0 = X_train_0.sample(frac = purchase_proportion).sort_index()
y_train_0 = y_train_0[y_train_0.index.isin(X_train_0.index)].sort_index()
print 'Undersampled number of cases for class "0" in X_train:', X_train_0.shape[0] 
print 'Undersampled number of cases for class "0" in y_train:', y_train_0.shape[0]


print 'X_train shape before undersampling:', X_train.shape[0]
print 'y_train shape before undersampling:', y_train.shape[0]
X_train_undersampled = pd.concat([X_train_0, X_train_1]).sample(frac=1, random_state=0)
y_train_undersampled = pd.concat([y_train_0, y_train_1]).sample(frac=1, random_state=0)
print 'X_train shape after undersampling:', X_train_undersampled.shape[0]
print 'y_train shape after undersampling:', y_train_undersampled.shape[0]

print '\nCheck indexes:'
print X_train_undersampled.index.tolist()[0:5]
print y_train_undersampled.index.tolist()[0:5]
print X_train_undersampled.index.tolist()[-5:]
print y_train_undersampled.index.tolist()[-5:]
Undersampled number of cases for class "0" in X_train: 40962
Undersampled number of cases for class "0" in y_train: 40962
X_train shape before undersampling: 626157
y_train shape before undersampling: 626157
X_train shape after undersampling: 81945
y_train shape after undersampling: 81945

Check indexes:
[642072, 122902, 298083, 611038, 545629]
[642072, 122902, 298083, 611038, 545629]
[361696, 515633, 69282, 154248, 36525]
[361696, 515633, 69282, 154248, 36525]
4.6) TEST DIFFERENT CLASSIFIERS ON UNDERSAMPLED DATA SETS:
In [18]:
# classifiers to test
alg = [SGDClassifier(), Perceptron(), \
       LogisticRegression(), RandomForestClassifier(), \
       LinearSVC(), GradientBoostingClassifier()]

classifier_stats = test_classifiers(alg, X_train_undersampled, y_train_undersampled, X_test, y_test)


Training SGDClassifier ...
Time needed for SGDClassifier is: 0.36 s
Classification report:
             precision    recall  f1-score   support

non-default       0.98      0.89      0.93     65064
    default       0.30      0.67      0.41      4509

avg / total       0.93      0.88      0.90     69573




Training Perceptron ...
Time needed for Perceptron is: 0.29 s
Classification report:
             precision    recall  f1-score   support

non-default       0.98      0.76      0.86     65064
    default       0.18      0.73      0.29      4509

avg / total       0.92      0.76      0.82     69573




Training LogisticRegression ...
Time needed for LogisticRegression is: 1.46 s
Classification report:
             precision    recall  f1-score   support

non-default       0.98      0.88      0.92     65064
    default       0.29      0.71      0.41      4509

avg / total       0.93      0.87      0.89     69573




Training RandomForestClassifier ...
Time needed for RandomForestClassifier is: 1.7 s
Classification report:
             precision    recall  f1-score   support

non-default       0.98      0.85      0.91     65064
    default       0.27      0.79      0.41      4509

avg / total       0.94      0.85      0.88     69573




Training LinearSVC ...
Time needed for LinearSVC is: 18.68 s
Classification report:
             precision    recall  f1-score   support

non-default       0.98      0.88      0.92     65064
    default       0.28      0.69      0.40      4509

avg / total       0.93      0.87      0.89     69573




Training GradientBoostingClassifier ...
Time needed for GradientBoostingClassifier is: 22.99 s
Classification report:
             precision    recall  f1-score   support

non-default       0.98      0.83      0.90     65064
    default       0.24      0.79      0.37      4509

avg / total       0.93      0.82      0.86     69573


SORT CLASSIFIERS BASED ON THE RECALL FOR CLASS 1
In [19]:
classifier_stats.sort_values('class 1 recall', ascending=False).head()[['name','weighted f1-score','class 1 recall','AUC_score','time']]
Out[19]:
name  weighted f1-score class 1 recall  AUC_score time
3 RandomForestClassifier  0.88  0.79  0.90  1.70
5 GradientBoostingClassifier  0.86  0.79  0.89  22.99
1 Perceptron  0.82  0.73  0.83  0.29
2 LogisticRegression  0.89  0.71  0.87  1.46
4 LinearSVC 0.89  0.69  0.87  18.68
4.7) PICK THE BEST CLASSIFIER AND EVALUATE ITS PERFORMANCE
In [20]:
classifier = RandomForestClassifier()
time_start = time.time()
classifier.fit(X_train_undersampled[model_columns], y_train_undersampled)
time_stop = time.time()
t = round((time_stop - time_start)/60,2)
print 'Time needed to train the classifier (min):', t
Time needed to train the classifier (min): 0.03
CALCULATE METRICS FOR THE TRAINING AND THE TEST SET
In [21]:
y_pred = metrics_for_train_and_test(classifier, X_train_undersampled, y_train_undersampled,\
                                    X_test, y_test, model_columns)
Metrics for the training set:
Accuracy: 0.99
AUC: 1.0
             precision    recall  f1-score   support

          0       0.98      1.00      0.99     40962
          1       1.00      0.98      0.99     40983

avg / total       0.99      0.99      0.99     81945



Metrics for the test set:
Accuracy: 0.85
AUC: 0.89
             precision    recall  f1-score   support

          0       0.98      0.85      0.91     65064
          1       0.27      0.78      0.40      4509

avg / total       0.94      0.85      0.88     69573

In [22]:
plot_confusion_matrix(y_test, y_pred, classes=['no purchase', 'purchase'],
                      title='Confusion matrix')
<matplotlib.figure.Figure at 0x10ecff290>

CHECK THE BUSINESS POTENTIAL OF THIS CLASSIFIER
In [23]:
estimate_profit_and_cost(X_test, y_test, y_pred, FP_cost=0.1)
Number of correctly classified potential buyers: 3531
Total estimated profit is: 524181.79
Number of incorrectly classified potential buyers: 9456
Total estimated cost due to False Positives is: 945.6
PLOT FEATURE IMPORTANCE
In [24]:
plot_feature_importance(classifier, model_columns)
<matplotlib.figure.Figure at 0x10fbc0d50>

VISUALIZE ONE OF THE CLASSIFIER'S TREES ON THE PLOT
In [25]:
export_graphviz(classifier.estimators_[0], feature_names=model_columns,
                max_depth = 3, filled=True, rounded=True, out_file='output/tree.dot')
os.system('dot -Tpng output/tree.dot -o output/tree.png')
Out[25]:
0
4.8) EVALUATE THE PERFORMANCE ON THE FULL VARIABLES SET
DECISION TREES CAN HANDLE THE CATEGORICAL VARIABLES, TRY IF IT HELPS
In [26]:
model_columns = [u'auction_duration', u'start_price', u'total_bids',
       u'first_2d_bids', u'last_2d_bids', u'final_price',
       u'final_price_cat_pctl', u'last_7d_searches', u'last_7d_item_views',
       u'last_7d_purchases', u'last_2d_searches', u'last_2d_item_views',
       u'last_2d_purchases', u'buyer_segment_num',
       u'full_category_num', u'category_num', 'parent_category_num']
In [27]:
classifier = RandomForestClassifier()
time_start = time.time()
classifier.fit(X_train_undersampled[model_columns], y_train_undersampled)
time_stop = time.time()
t = round((time_stop - time_start)/60,2)
print 'Time needed to train the classifier (min):', t
Time needed to train the classifier (min): 0.03
CALCULATE METRICS FOR THE TRAINING AND THE TEST SET
In [28]:
y_pred = metrics_for_train_and_test(classifier, X_train_undersampled, y_train_undersampled,\
                                    X_test, y_test, model_columns)
Metrics for the training set:
Accuracy: 0.99
AUC: 1.0
             precision    recall  f1-score   support

          0       0.98      1.00      0.99     40962
          1       1.00      0.98      0.99     40983

avg / total       0.99      0.99      0.99     81945



Metrics for the test set:
Accuracy: 0.86
AUC: 0.9
             precision    recall  f1-score   support

          0       0.98      0.86      0.92     65064
          1       0.29      0.81      0.43      4509

avg / total       0.94      0.86      0.89     69573

In [29]:
plot_confusion_matrix(y_test, y_pred, classes=['no purchase', 'purchase'],
                      title='Confusion matrix')
<matplotlib.figure.Figure at 0x1148ce350>

CHECK THE BUSINESS POTENTIAL OF THIS CLASSIFIER
In [30]:
estimate_profit_and_cost(X_test, y_test, y_pred, FP_cost=0.1)
Number of correctly classified potential buyers: 3634
Total estimated profit is: 539820.27
Number of incorrectly classified potential buyers: 8834
Total estimated cost due to False Positives is: 883.4
PLOT FEATURE IMPORTANCE
In [31]:
plot_feature_importance(classifier, model_columns)
<matplotlib.figure.Figure at 0x11356e490>

5) GRID SEARCH ON THE PARAMETERS
5.1) DECLARE THE GRID OF THE PARAMETERS FOR THE SCAN
In [32]:
param_grid = {'n_estimators': range(10, 101, 10)} 
#param_grid = {'max_depth': range(3, 21, 2)}
#param_grid = {'max_features': range(1, 19, 2)}
5.2) PERFORM GRID SEARCH - IT CAN TAKE A WHILE
In [33]:
time_start = time.time()
classifier = RandomForestClassifier()
grid_s = GridSearchCV(classifier, param_grid, scoring='recall_weighted', cv=3, verbose=0)
grid_s.fit(X_train_undersampled[model_columns], y_train_undersampled)
time_stop = time.time()
t = round((time_stop - time_start)/60,2)
print 'Time needed for the grid search (min):', t
grid_s.grid_scores_, grid_s.best_params_, grid_s.best_score_
Time needed for the grid search (min): 3.88
/Users/Madzias/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20
  DeprecationWarning)
Out[33]:
([mean: 0.81979, std: 0.00037, params: {'n_estimators': 10},
  mean: 0.82745, std: 0.00171, params: {'n_estimators': 20},
  mean: 0.83186, std: 0.00247, params: {'n_estimators': 30},
  mean: 0.83249, std: 0.00198, params: {'n_estimators': 40},
  mean: 0.83324, std: 0.00228, params: {'n_estimators': 50},
  mean: 0.83291, std: 0.00099, params: {'n_estimators': 60},
  mean: 0.83413, std: 0.00117, params: {'n_estimators': 70},
  mean: 0.83540, std: 0.00248, params: {'n_estimators': 80},
  mean: 0.83512, std: 0.00167, params: {'n_estimators': 90},
  mean: 0.83502, std: 0.00196, params: {'n_estimators': 100}],
 {'n_estimators': 80},
 0.83540179388614311)
###### COMMENTS: ###### Increasing the number of estimators in the forest help improving the recall for the class 1 #######################
5.3) TEST GRID SEARCH
In [34]:
y_pred = metrics_for_train_and_test(grid_s, X_train_undersampled, y_train_undersampled,\
                                    X_test, y_test, model_columns)
Metrics for the training set:
Accuracy: 1.0
AUC: 1.0
             precision    recall  f1-score   support

          0       1.00      1.00      1.00     40962
          1       1.00      1.00      1.00     40983

avg / total       1.00      1.00      1.00     81945



Metrics for the test set:
Accuracy: 0.86
AUC: 0.92
             precision    recall  f1-score   support

          0       0.99      0.86      0.92     65064
          1       0.29      0.83      0.43      4509

avg / total       0.94      0.86      0.89     69573

In [35]:
plot_confusion_matrix(y_test, y_pred, classes=['no purchase', 'purchase'],
                      title='Confusion matrix')
<matplotlib.figure.Figure at 0x115f9f2d0>

###### COMMENTS: ###### After finding a good set of parameters, feed them to the chosen classifier #######################
6) FEATURE SELECTION
CHECK MODEL PERFORMANCE WHEN FILTERING FEATURES BASED ON THE IMPORTANCE THRESHOLD
In [37]:
classifier.fit(X_train_undersampled[model_columns], y_train_undersampled)
for threshold in sorted(classifier.feature_importances_)[-10:]:
    # filter features using threshold from the feature importance
    selection = SelectFromModel(classifier, threshold=threshold, prefit=True)
    select_X_train = selection.transform(X_train_undersampled[model_columns])
    # train selection model
    selection_model = RandomForestClassifier()
    selection_model.fit(select_X_train, y_train_undersampled)
    # evaluate selection model
    select_X_test = selection.transform(X_test[model_columns])
    y_pred = selection_model.predict(select_X_test)
    # calculate the recall for class 1
    recall = round(recall_score(y_test, y_pred),2)
    # print the results
    print("Thresh=%.3f, n=%d, Class 1 recall: %.2f%%" % (threshold, select_X_train.shape[1], recall*100.0))
Thresh=0.051, n=10, Class 1 recall: 79.00%
Thresh=0.053, n=9, Class 1 recall: 79.00%
Thresh=0.054, n=8, Class 1 recall: 80.00%
Thresh=0.055, n=7, Class 1 recall: 79.00%
Thresh=0.058, n=6, Class 1 recall: 79.00%
Thresh=0.060, n=5, Class 1 recall: 77.00%
Thresh=0.060, n=4, Class 1 recall: 76.00%
Thresh=0.096, n=3, Class 1 recall: 71.00%
Thresh=0.110, n=2, Class 1 recall: 71.00%
Thresh=0.220, n=1, Class 1 recall: 78.00%
In [ ]:


